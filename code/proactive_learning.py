"""
What triggered that emotion? Emotion cause extraction in conversational discourse.
Copyright (C) 2022  Rigved Rakshit

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU Affero General Public License as published
by the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU Affero General Public License for more details.

You should have received a copy of the GNU Affero General Public License
along with this program.  If not, see <https://www.gnu.org/licenses/>.
"""


import os
import sys
from tqdm.auto import tqdm
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
from sklearn.base import BaseEstimator, ClassifierMixin
from sklearn.utils.validation import check_is_fitted
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_recall_fscore_support
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from transformers import get_scheduler, AdamW
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
import torch
from modAL.models import ActiveLearner
from modAL.uncertainty import uncertainty_sampling, margin_sampling, entropy_sampling


class EmotionsDataset(Dataset):
    """
    Convert the data into a map-style dataset that can be plugged
    into a data loader for the transformer model.
    """
    def __init__(
            self,
            text,
            labels,
            tokenizer=AutoTokenizer.from_pretrained('facebook/bart-large-mnli')
    ):
        """
        Tokenize the variable-length text (premise + hypothesis)
        as a list of variable-length tokenized tensors; and store
        the corresponding labels.

        :param text: The premise and the hypothesis statements.
        :param labels: The labels for this textual entailment model.
                       It is '1' if the premise entails the hypothesis,
                       and '0' otherwise.
        :param tokenizer: The pretrained tokenizer associated with the
                           pretrained model that will be applied on this dataset.
        """
        self.encodings = {
            'input_ids': list(),
            'attention_mask': list(),
            'labels': list()
        }

        # Padding is delayed until a batch is generated by the
        # data loader, at which point, dynamic padding is applied.
        # For this reason, each sentence of the text needs to be
        # tokenized individually.
        for data_index in range(text.shape[0]):
            tokenized_text = tokenizer(
                text.iloc[data_index, 0],
                text.iloc[data_index, 1],
                return_tensors='pt',
                truncation=False,
                padding=False
            )

            self.encodings['input_ids'].append(tokenized_text['input_ids'])
            self.encodings['attention_mask'].append(tokenized_text['attention_mask'])
            self.encodings['labels'].append(torch.tensor([labels.iloc[data_index]]))

    def __len__(self):
        """
        Calculate the total number of text documents.

        :return: The total number of text documents.
        """
        return len(self.encodings['input_ids'])

    def __getitem__(self, idx):
        """
        Allow Python-style [] indexing.

        :param idx: Index of the item to retrieve.

        :return: The requested item from the list of tokenized texts.
        """
        return {k: v[idx] for k, v in self.encodings.items()}


class BARTEmotionLearner(BaseEstimator, ClassifierMixin):
    """
    Active Learner based on BART-Large-MNLI textual entailment model checkpoint.
    """
    def __init__(
            self,
            plutchik_emotions=[
                'Acceptance',
                'Aggressiveness',
                'Annoyance',
                'Apprehension',
                'Awe',
                'Boredom',
                'Contempt',
                'Disapproval',
                'Distraction',
                'Friendliness',
                'Hostility'
                'Interest',
                'Optimism',
                'Pensiveness',
                'Remorse',
                'Serenity',
                'Submission',
            ],
            oracles=[
                'models/easy_to_difficult_sampling',
                'models/entropy_sampling',
                'models/margin_sampling',
                'models/uncertainty_sampling'
            ],
            reliability_scores=[
                0.0,
                0.0,
                0.0,
                0.0
            ],
            proactive_learning_model_checkpoint='models/proactive_learning_sampling',
            tokenizer=AutoTokenizer.from_pretrained('facebook/bart-large-mnli'),
            model=AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli'),
            device=torch.device('cuda:0'),
            batch_size=8,
            lr=5e-5,
            num_warmup_steps=0,
            n_iter=50,
            verbose=True
    ):
        """
        Initialize the underlying BART-Large-MNLI checkpoint and prepare the data.

        :param plutchik_emotions: Emotion labels from Plutchik's wheel of emotions.
        :param oracles: The list of Proactive Learning oracles. Not used in the Active Learning case.
        :param reliability_scores: The reliability scores of each of the Proactive Learning oracles.
                                   Not used in the Active Learning case.
        :param proactive_learning_model_checkpoint: The current Proactive Learning model's checkpoint name.
                                                             This helps to load the model after it was unloaded
                                                             temporarily to accommodate the oracle's model on the
                                                             single instance of the GPU.
                                                             Not used in the Active Learning case.
        :param tokenizer: BART textual entailment model's tokenizer. Default checkpoint: facebook/bart-large-mnli.
        :param model: BART textual entailment model. Default checkpoint: facebook/bart-large-mnli.
                      The BART pretrained model is configured to output 3 labels by default:
                          0. Contradiction
                          1. Neutral
                          2. Entailment
        :param device: The CUDA-enabled GPU device to use for this model. Default: 1st available CUDA-enabled GPU.
        :param batch_size: The number of instances in each batch.
                           Default: 8 (This batch size fits into the memory offered by an NVIDIA A100.)
        :param lr: Learning rate for the transformer head's optimizer. Default: 5e-5
        :param num_warmup_steps: Number of warmup steps for the learning rate scheduler during training.
        :param n_iter: Number of epochs for the training. Default: 10
        :param verbose: Output training progress and training loss.
        """
        self.plutchik_emotions = plutchik_emotions
        self.oracles = oracles
        self.reliability_scores = np.array(reliability_scores)
        self.proactive_learning_model_checkpoint = proactive_learning_model_checkpoint
        self.tokenizer = tokenizer
        self.device = device
        self.model = model
        self.model.to(self.device)
        self.batch_size = batch_size
        self.lr = lr
        self.n_iter = n_iter
        self.num_warmup_steps = num_warmup_steps
        self.verbose = verbose

    def fit(self, X, y):
        """
        Incrementally fine-tune the underlying BART textual entailment model.

        :param X: A Pandas series consisting of the premise text.
        :param y: A Pandas series for the corresponding emotion label
                  for textual entailment.

        :return: Fitted model.
        """
        # Store the training loss so that it can be used later
        self.training_loss_ = list()

        # Store the oracle selection history for review later
        self.oracle_selection_history_ = list()

        # Default optimizer for the training
        optimizer = AdamW(self.model.parameters(), lr=self.lr)

        # Create the data loader for the training instances.
        dataloader_train = self.prepare_data(X, y, shuffle=True)

        # Number of steps for the training
        num_training_steps = self.n_iter * len(dataloader_train)

        # Learning rate scheduler
        lr_scheduler = get_scheduler(
            'linear',
            optimizer=optimizer,
            num_warmup_steps=self.num_warmup_steps,
            num_training_steps=num_training_steps
        )

        if self.verbose:
            # Set up the progress bar
            progress_bar = tqdm(range(num_training_steps))

        # Train the model
        self.model.train()
        for epoch in range(self.n_iter):
            epoch_loss = 0.0

            for batch in dataloader_train:
                outputs = self.model(**batch)
                outputs.loss.backward()
                optimizer.step()
                lr_scheduler.step()
                optimizer.zero_grad()
                epoch_loss += outputs.loss.item()

                if self.verbose:
                    progress_bar.update(1)

            epoch_loss = epoch_loss / len(dataloader_train)
            self.training_loss_.append(epoch_loss)

            if self.verbose:
                print(f'Epoch #{epoch + 1} loss: {epoch_loss}')

        return self

    def predict(self, X):
        """
        Predict the emotion detected in the given text.

        :param X: A Pandas series consisting of the premise text.

        :return Predicted labels
        """
        # Ensure that fit has been called before attempting predictions
        check_is_fitted(
            self,
            attributes=[
                'training_loss_',
                'oracle_selection_history_'
            ]
        )

        # Find the most likely labels
        probabilities = self.predict_proba(X)

        return (np.array(self.plutchik_emotions, dtype=np.str_)[np.argmax(probabilities, axis=1)]).tolist()

    def predict_proba(self, X):
        """
        Calculate the probabilities of textual contradiction and entailment
        for the given text.

        :param X: A Pandas series consisting of the premise text.

        :return: Probabilities for each label, with shape (X.shape[0], n_classes)
        """
        # Ensure that fit has been called before attempting predictions
        check_is_fitted(
            self,
            attributes=[
                'training_loss_',
                'oracle_selection_history_'
            ]
        )

        # Create the dataloader for the testing instances
        dataloader_test = self.prepare_data(X, shuffle=False)

        # Record the prediction logits
        entailment_logits = list()

        self.model.eval()
        for batch in dataloader_test:
            with torch.no_grad():
                outputs = self.model(**batch)

                # We throw away the 'contradiction' (dim 0) and the 'neutral' (dim 1)
                # dimensions and take the logits of 'entailment' (2).
                for logit in outputs.logits[:, 2]:
                    entailment_logits.append(logit)

        # Convert the entailment logits into class probabilities
        probabilities = list()

        # The logits for a single premise are stored in contiguous indexes in the
        # 'entailment_logits'. So, if there are 17 Plutchik emotions, then the first
        # 17 indexes (0-16) correspond to the logits for these 17 emotion labels for the
        # first premise text. Then, the next 17 index (17-33) correspond to the logits for the
        # second premise text. And so on.
        #
        # In other words, we convert:
        #   [premise_1_logit_0,
        #    premise_1_logit_1,
        #    ...,
        #    premise_1_logit_16,
        #    premise_2_logit_0,
        #    premise_2_logit_1,
        #    ...,
        #    premise_2_logit_16,
        #    ...,
        #    premise_n_logit_16]
        #
        # to:
        #
        #   [[premise_1_logit_0, premise_1_logit_1, ..., premise_1_logit_16],
        #    [premise_2_logit_0, premise_2_logit_1, ..., premise_2_logit_16],
        #    ...,
        #    [premise_n_logit_0, premise_n_logit_1, ..., premise_n_logit_16]]
        for probability_start_index in range(0, len(entailment_logits), len(self.plutchik_emotions)):
            probabilities.append(
                entailment_logits[probability_start_index:(probability_start_index + len(self.plutchik_emotions))]
            )

        return torch.tensor(probabilities).to(self.device).softmax(dim=1).detach().cpu().numpy()

    def prepare_data(self, X, y=None, shuffle=True):
        """
        Convert the given text into a Pandas DataFrame consisting of the
        premise and the corresponding hypothesis.

        :param X: A Pandas series consisting of the premise text.
        :param y: A Pandas series for the corresponding emotion label
                  for textual entailment. If the labels are not
                  provided, then they are assumed to be '1' (Neutral) - neither
                  contradiction nor entailment.
        :param shuffle: True (default) if the dataloader should shuffle the instances;
                        False otherwise.

        :return: A PyTorch DataLoader consisting of the premise, hypothesis,
                 and the labels as tokenized GPU tensors.
        """
        # Prepare the data
        if y is not None:
            df = pd.DataFrame({
                'text': X,
                'labels': y
            })
        else:
            df = pd.DataFrame({
                'text': X
            })

        # For each emotion that's displayed, generate its premise and hypothesis
        premises = list()
        hypotheses = list()
        labels = list()

        # Structure of each row: (Text, Labels) or (Text,)
        for row in df.itertuples(index=False):
            # We add a hypothesis for each emotion for each premise text.
            #
            # In other words, we convert:
            #   [premise_1,
            #    premise_2,
            #    ...,
            #    premise_n]
            #
            # to:
            #   [[premise_1, hypothesis_emotion_0],
            #    [premise_1, hypothesis_emotion_1],
            #    ...,
            #    [premise_1, hypothesis_emotion_16],
            #    [premise_2, hypothesis_emotion_1],
            #    [premise_2, hypothesis_emotion_2],
            #    ...,
            #    [premise_n, hypothesis_emotion_16]]
            for emotion in self.plutchik_emotions:
                premises.append(row[0])
                hypotheses.append(f'The emotion in this example is {emotion}.')

                # Select the class labels:
                #   0: Contradiction
                #   1: Neutral (if the class labels are not known)
                #   2: Entailment
                if y is not None:
                    if emotion == row[1]:
                        labels.append(2)
                    else:
                        labels.append(0)
                else:
                    labels.append(1)

        df = pd.DataFrame({
            'premise': premises,
            'hypothesis': hypotheses,
            'labels': labels
        })

        return DataLoader(
            EmotionsDataset(
                df[['premise', 'hypothesis']],
                df['labels'],
                self.tokenizer
            ),
            shuffle=shuffle,
            batch_size=self.batch_size,
            collate_fn=self.collate_batch
        )

    def collate_batch(self, batch):
        """
        Data collator function that takes a batch of variable-length tokenized
        tensors (variable-length tokenized input strings of sizes 'size_1',
        'size_2', ..., 'size_n') and pads them to be the same length as the
        largest sequence of tokens in the batch. Here, 'n' is the current
        batch size.
        Since tensors on GPUs need to be of the same length, the input
        variable-length tensors are stored on the CPU as a list of tensors,
        padded, and then moved to the first available GPU.

        :param batch: List of variable length tokenized text. Represented as
                      batch = [
                        {
                            'input_ids': Tensor[1, size_1],
                            'attention_mask': Tensor[1, size_1],
                            'labels: Tensor[1]'
                        },
                        {
                            'input_ids': Tensor[1, size_2],
                            'attention_mask': Tensor[1, size_2],
                            'labels: Tensor[1]'
                        },
                        ...,
                        {
                            'input_ids': Tensor[1, size_n],
                            'attention_mask': Tensor[1, size_n],
                            'labels: Tensor[1]'
                        }
                      ]
        :return: Tokenized text with padding. Represented as
                 return {
                    'input_ids': Tensor[batch_size, max_padded_size_of_this_batch],
                    'attention_mask': Tensor[batch_size, max_padded_size_of_this_batch],
                    'labels': Tensor[batch_size]
                 }
        """
        # Extract all the 'input_ids', 'attention_mask', and 'labels' tensors for all
        # the items in this batch
        input_ids = list()
        attention_mask = list()
        labels = list()

        for item in batch:
            # The extracted tensors are 1-D row tensors.
            # However, the function in Torch only pads 1-D column tensors.
            # Therefore, transpose the 1-D row tensors into 1-D column tensors.
            input_ids.append(torch.transpose(item['input_ids'], 0, 1))
            attention_mask.append(torch.transpose(item['attention_mask'], 0, 1))
            labels.append(item['labels'])

        # After padding the tensors to the same length, re-transpose them back into
        # 1-D row tensors.
        input_ids = torch.transpose(pad_sequence(input_ids, batch_first=True, padding_value=0), 1, 2)
        attention_mask = torch.transpose(pad_sequence(attention_mask, batch_first=True, padding_value=0), 1, 2)

        # The HuggingFace DataLoader expects tensors of shape [batch_size, max_padded_size_of_this_batch]
        # for the 'input_ids' and the 'attention_mask'. Therefore, reshape the Tensors from
        # [batch_size, 1, max_padded_size_of_this_batch] to [batch_size, max_padded_size_of_this_batch].
        # This is required because we originally used a list of variable length 1-D row tensors.
        return {
            'input_ids': torch.reshape(input_ids, (input_ids.shape[0], input_ids.shape[2])).to(self.device),
            'attention_mask': torch.reshape(attention_mask, (input_ids.shape[0], attention_mask.shape[2])).to(self.device),
            'labels': torch.tensor(labels).to(self.device)
        }

    def score(self, y_true, y_pred, **kwargs):
        """
        Calculate the Precision, Recall, F1 score, and the Accuracy
        of the predictions vs the gold standard.

        :param y_pred: The predictions from the model.
        :param y_true: The ground truth labels.
        :param kwargs: Keyword arguments for the "ClassifierMinix.score" method.
                       By default, it can accept optional array-like sample
                       weights of shape (n_samples,). Default: None

        :return: Precision, Recall, F1 score, and Accuracy as a dictionary.
        """
        precision, recall, f1, _ = precision_recall_fscore_support(
            y_true,
            y_pred,
            labels=self.plutchik_emotions,
            average='weighted'
        )
        accuracy = super().score(y_true, y_pred)

        return {
            'Accuracy': accuracy,
            'F1': f1,
            'Precision': precision,
            'Recall': recall
        }


def easy_to_difficult_query_strategy(
        classifier,
        instances,
        n_instances=100,
):
    """
    Custom Active Learning instance selection strategy. This query finds the utility
    of each instance in the pool and picks the most informative instances to use in the
    next iteration of training. Instances that are easier to label are returned first.

    :param classifier: The Active Learner. Since this method automatically gets added to
                       the Active Learner class, this parameter is the equivalent of
                       'self' for this Active Learner method.
    :param instances: The list of training instances in the pool from which the
                      next most informative instances will be picked.
    :param n_instances: Number of informative instances to pick.

    :return: The indices of the instances from X chosen to be labelled
             and the instances from X chosen to be labelled.
    """
    # Find the next set of informative instances for the training from the given pool split
    prediction_probabilities = classifier.estimator.predict_proba(instances)
    sorted_probability_indexes = np.amax(prediction_probabilities, axis=1).argsort()

    return sorted_probability_indexes[-n_instances:]


def proactive_learning_query_strategy(
        classifier,
        instances,
        n_instances=100
):
    """
    Custom Proactive Learning instance selection strategy. This query finds the utility
    of each instance in the pool using the selected oracle and picks the most
    informative instances to use in the next iteration of training. Depending on the
    oracle that was currently picked, the instances that are selected could differ
    drastically.

    :param classifier: The Active Learner. Since this method automatically gets added to
                       the Active Learner class, this parameter is the equivalent of
                       'self' for this Active Learner method.
    :param instances: The list of training instances in the pool from which the
                      next most informative instances will be picked.
    :param n_instances: Number of informative instances to pick.

    :return: The indices of the instances from X chosen to be labelled
             and the instances from X chosen to be labelled.
    """
    # Find the next set of informative instances for the training from the given pool split
    prediction_probabilities = classifier.estimator.predict_proba(instances)
    sorted_probability_indexes = np.amax(prediction_probabilities, axis=1).argsort()

    return sorted_probability_indexes[:n_instances]


def run_proactive_learning_experiment(
        learner,
        instances_x_pool,
        instances_y_pool,
        instances_x_val,
        instances_y_val,
        n_query_iterations=50,
        n_instances_pool=100
):
    """
    Proactive Learning main loop. It picks instances to learn and iteratively trains
    the Active learner.

    :param learner: The Proactive Learner that'll be further trained.
    :param instances_x_pool: The pool of instances to use for further training.
    :param instances_y_pool: The pool of instance labels to use for further training.
    :param instances_x_val: The validation split used for evaluation.
    :param instances_y_val: The labels of the validation split used for evaluation.
    :param n_query_iterations: Number of queries for the Active Learning experiment.
    :param n_instances_pool: Number of instances from the pool split to pick
                             in each Active Learning iteration.

    :return: Precision, Recall, F1 score, and Accuracy on the validation data split
             after the current Active Learning iteration completes.
    """
    # Store the performance history
    learner_performance_history = list()

    # Use the best available oracle at the start
    current_best_oracle_index = learner.estimator.reliability_scores.shape[0] - 1
    learner.estimator.oracle_selection_history_.append(current_best_oracle_index)

    for index in range(n_query_iterations):
        if learner.estimator.verbose:
            print(f'Proactive Learning Epoch {index + 1}...')

        # Find the current oracle
        current_best_oracle = np.argpartition(learner.estimator.reliability_scores, current_best_oracle_index)[current_best_oracle_index]

        # Swap the current model with the oracle's model
        learner.estimator.model.save_pretrained(learner.estimator.proactive_learning_model_checkpoint)
        del learner.estimator.model
        learner.estimator.model = AutoModelForSequenceClassification.from_pretrained(learner.estimator.oracles[current_best_oracle])
        learner.estimator.model.to(learner.estimator.device)

        # Find the next informative indexes to use for training
        query_indexes, _ = learner.query(
            instances_x_pool,
            n_instances=n_instances_pool
        )

        # Re-load the Proactive Learning model
        del learner.estimator.model
        learner.estimator.model = AutoModelForSequenceClassification.from_pretrained(learner.estimator.proactive_learning_model_checkpoint)
        learner.estimator.model.to(learner.estimator.device)

        # Fine-tune the Proactive Learner using the selected instance(s)
        learner.teach(
            X=instances_x_pool.iloc[query_indexes],
            y=instances_y_pool.iloc[query_indexes],
            only_new=True
        )

        # Store the performance of the learner after the new learning phase.
        # Use the validation dataset for this purpose.
        performance = learner.score(instances_y_val, learner.predict(instances_x_val))
        learner_performance_history.append([performance[key] for key in ['Precision', 'Recall', 'F1', 'Accuracy']])

        # Pick the next oracle such that the cost is minimized using a greedy approach
        if performance['F1'] > learner.estimator.reliability_scores[current_best_oracle]:
            # If the current model performed better than the oracle, then choose a less
            # reliable oracle next time
            if current_best_oracle_index != 0:
                current_best_oracle_index -= 1
        elif performance['F1'] < learner.estimator.reliability_scores[current_best_oracle]:
            # If the current model performed worse than the oracle, then choose a more
            # reliable oracle next time
            if current_best_oracle_index != (learner.estimator.reliability_scores.shape[0] - 1):
                current_best_oracle_index += 1

        learner.estimator.oracle_selection_history_.append(current_best_oracle_index)

    return learner_performance_history


def run_active_learning_experiment(
        learner,
        instances_x_pool,
        instances_y_pool,
        instances_x_val,
        instances_y_val,
        n_query_iterations=50,
        n_instances_pool=100
):
    """
    Active Learning main loop. It picks instances to learn and iteratively trains
    the Active learner.

    :param learner: The Active Learner that'll be further trained.
    :param instances_x_pool: The pool of instances to use for further training.
    :param instances_y_pool: The pool of instance labels to use for further training.
    :param instances_x_val: The validation split used for evaluation.
    :param instances_y_val: The labels of the validation split used for evaluation.
    :param n_query_iterations: Number of queries for the Active Learning experiment.
    :param n_instances_pool: Number of instances from the pool split to pick
                             in each Active Learning iteration.

    :return: Precision, Recall, F1 score, and Accuracy on the validation data split
             after the current Active Learning iteration completes.
    """
    # Store the performance history
    learner_performance_history = list()

    for index in range(n_query_iterations):
        if learner.estimator.verbose:
            print(f'Active Learning Epoch {index + 1}...')

        # Find the next informative indexes to use for training
        query_indexes, _ = learner.query(
            instances_x_pool,
            n_instances=n_instances_pool
        )

        # Fine-tune the Active Learner using the selected instance(s)
        learner.teach(
            X=instances_x_pool.iloc[query_indexes],
            y=instances_y_pool.iloc[query_indexes],
            only_new=True
        )

        # Store the performance of the learner after the new learning phase.
        # Use the validation dataset for this purpose.
        performance = learner.score(instances_y_val, learner.predict(instances_x_val))
        learner_performance_history.append([performance[key] for key in ['Precision', 'Recall', 'F1', 'Accuracy']])

    return learner_performance_history


if __name__ == '__main__':
    # Disable parallelism in the tokenizers because it does not support forking the parent
    # Python process after the tokenizers have already been run at least once.
    # This does not cause any performance issues because the dataset is very small.
    os.environ['TOKENIZERS_PARALLELISM'] = 'false'

    # Set options for pandas display
    pd.set_option('display.max_columns', None)
    pd.set_option('display.max_colwidth', None)
    pd.set_option('display.max_seq_item', None)
    pd.set_option('display.max_rows', None)
    pd.set_option('display.precision', 2)

    # Seed for various random number generators
    seed = 42

    # The BART model needs the memory offered by at least an NVIDIA A100 GPU
    if torch.cuda.is_available():
        # Set the PyTorch seed for reproducibility
        torch.cuda.manual_seed_all(seed)
    else:
        print('The underlying BART-Large-MNLI model\'s training needs the memory offered '
              'by at least an NVIDIA A100 GPU!')
        sys.exit(1)

    # Set the Numpy random generator seed for reproducibility
    np.random.seed(seed)

    # Load the dataset
    raw_dataset = pd.read_csv('../data/emotion_data_annotated_by_humans.csv')

    # Extract the required features and split into train, pool, validation, and test datasets
    data = pd.DataFrame({
        'text': pd.concat(
            [
                raw_dataset['question'],
                raw_dataset['response'],
                raw_dataset['question'],
                raw_dataset['response']
            ],
            ignore_index=True
        ),
        'labels': pd.concat(
            [
                raw_dataset['q_emotion1'],
                raw_dataset['r_emotion1'],
                raw_dataset['q_emotion2'],
                raw_dataset['r_emotion2']
            ],
            ignore_index=True
        )
    })

    # Remove empty data for emotion labels
    data.dropna(inplace=True)

    # Split the dataset into training, pool, validation, and testing datasets
    x_train, x_val, y_train, y_val = train_test_split(
        data['text'],
        data['labels'],
        train_size=0.9,
        random_state=42,
        stratify=data['labels']
    )
    x_train, x_test, y_train, y_test = train_test_split(
        x_train,
        y_train,
        train_size=0.9,
        random_state=42,
        stratify=y_train
    )
    x_train, x_pool, y_train, y_pool = train_test_split(
        x_train,
        y_train,
        train_size=0.5,
        random_state=42,
        stratify=y_train
    )

    # Cannot reset_index inplace on a Series to create a DataFrame
    x_train = x_train.reset_index(drop=True)
    y_train = y_train.reset_index(drop=True)
    x_pool = x_pool.reset_index(drop=True)
    y_pool = y_pool.reset_index(drop=True)
    x_val = x_val.reset_index(drop=True)
    y_val = y_val.reset_index(drop=True)
    x_test = x_test.reset_index(drop=True)
    y_test = y_test.reset_index(drop=True)

    # We restrict to using a single GPU so that the SLURM job running this experiment
    # gets picked up faster. We will test each Active Learning and Proactive Learning
    # strategy one-by-one on the same dataset splits and then compare their performance.
    #
    # Store the performance of the Proactive Learners with each iteration of learning
    performance_history = {
        'Easy-to-Difficult Sampling': list(),
        'Easy-to-Difficult Sampling Loss': list(),
        'Entropy Sampling': list(),
        'Entropy Sampling Loss': list(),
        'Margin Sampling': list(),
        'Margin Sampling Loss': list(),
        'Uncertainty Sampling': list(),
        'Uncertainty Sampling Loss': list()
    }

    # Proactive Learning parameters for the number of instances to pick for each query
    # and the number of total queries to perform.
    n_iterations = 50
    n_pool_instances = 100
    bart_oracles = [
        'models/easy_to_difficult_sampling',
        'models/entropy_sampling',
        'models/margin_sampling',
        'models/uncertainty_sampling'
    ]

    # Clear GPU memory
    torch.cuda.empty_cache()

    # Strategy 1: Easy-to-Difficult Sampling
    bart_emotion_learner = BARTEmotionLearner()
    bart_active_learner = ActiveLearner(
        estimator=bart_emotion_learner,
        query_strategy=easy_to_difficult_query_strategy,
        X_training=x_train,
        y_training=y_train
    )
    performance_history['Easy-to-Difficult Sampling'].extend(run_active_learning_experiment(
        learner=bart_active_learner,
        instances_x_pool=x_pool,
        instances_y_pool=y_pool,
        instances_x_val=x_val,
        instances_y_val=y_val,
        n_query_iterations=n_iterations,
        n_instances_pool=n_pool_instances
    ))
    performance_history['Easy-to-Difficult Sampling Loss'].extend(bart_emotion_learner.training_loss_)
    bart_emotion_learner.model.save_pretrained(bart_oracles[0])

    print('Performance History (Easy-to-Difficult Sampling):')
    print(performance_history)

    # Clear GPU memory
    del bart_emotion_learner.model
    torch.cuda.empty_cache()

    # Strategy 2: Entropy Sampling
    bart_emotion_learner = BARTEmotionLearner()
    bart_active_learner = ActiveLearner(
        estimator=bart_emotion_learner,
        query_strategy=entropy_sampling,
        X_training=x_train,
        y_training=y_train
    )
    performance_history['Entropy Sampling'].extend(run_active_learning_experiment(
        learner=bart_active_learner,
        instances_x_pool=x_pool,
        instances_y_pool=y_pool,
        instances_x_val=x_val,
        instances_y_val=y_val,
        n_query_iterations=n_iterations,
        n_instances_pool=n_pool_instances
    ))
    performance_history['Entropy Sampling Loss'].extend(bart_emotion_learner.training_loss_)
    bart_emotion_learner.model.save_pretrained(bart_oracles[1])

    print('Performance History (Entropy Sampling):')
    print(performance_history)

    # Clear GPU memory
    del bart_emotion_learner.model
    torch.cuda.empty_cache()

    # Strategy 3: Margin Sampling
    bart_emotion_learner = BARTEmotionLearner()
    bart_active_learner = ActiveLearner(
        estimator=bart_emotion_learner,
        query_strategy=margin_sampling,
        X_training=x_train,
        y_training=y_train
    )
    performance_history['Margin Sampling'].extend(run_active_learning_experiment(
        learner=bart_active_learner,
        instances_x_pool=x_pool,
        instances_y_pool=y_pool,
        instances_x_val=x_val,
        instances_y_val=y_val,
        n_query_iterations=n_iterations,
        n_instances_pool=n_pool_instances
    ))
    performance_history['Margin Sampling Loss'].extend(bart_emotion_learner.training_loss_)
    bart_emotion_learner.model.save_pretrained(bart_oracles[2])

    print('Performance History (Margin Sampling):')
    print(performance_history)

    # Clear GPU memory
    del bart_emotion_learner.model
    torch.cuda.empty_cache()

    # Strategy 4: Uncertainty Sampling
    bart_emotion_learner = BARTEmotionLearner()
    bart_active_learner = ActiveLearner(
        estimator=bart_emotion_learner,
        query_strategy=uncertainty_sampling,
        X_training=x_train,
        y_training=y_train
    )
    performance_history['Uncertainty Sampling'].extend(run_active_learning_experiment(
        learner=bart_active_learner,
        instances_x_pool=x_pool,
        instances_y_pool=y_pool,
        instances_x_val=x_val,
        instances_y_val=y_val,
        n_query_iterations=n_iterations,
        n_instances_pool=n_pool_instances
    ))
    performance_history['Uncertainty Sampling Loss'].extend(bart_emotion_learner.training_loss_)
    bart_emotion_learner.model.save_pretrained(bart_oracles[3])

    print('Performance History (Uncertainty Sampling):')
    print(performance_history)

    # Clear GPU memory
    del bart_emotion_learner.model
    torch.cuda.empty_cache()

    # Strategy 5: Proactive Learning
    bart_reliability_scores = [performance_history[key][-1][2] for key in sorted(performance_history.keys()) if ' Loss' not in key]
    
    performance_history['Proactive Learning Sampling'] = list()
    performance_history['Proactive Learning Sampling Loss'] = list()
    
    bart_emotion_learner = BARTEmotionLearner(
        reliability_scores=bart_reliability_scores
    )
    bart_active_learner = ActiveLearner(
        estimator=bart_emotion_learner,
        query_strategy=proactive_learning_query_strategy,
        X_training=x_train,
        y_training=y_train
    )
    performance_history['Proactive Learning Sampling'].extend(run_proactive_learning_experiment(
        learner=bart_active_learner,
        instances_x_pool=x_pool,
        instances_y_pool=y_pool,
        instances_x_val=x_val,
        instances_y_val=y_val,
        n_query_iterations=n_iterations,
        n_instances_pool=n_pool_instances
    ))
    performance_history['Proactive Learning Sampling Loss'].extend(bart_emotion_learner.training_loss_)
    bart_emotion_learner.model.save_pretrained(bart_emotion_learner.proactive_learning_model_checkpoint)

    print('Performance History (Proactive Learning Sampling):')
    print(performance_history)
    print('Oracle Selection History:')
    print(bart_emotion_learner.oracle_selection_history_)

    # Clear GPU memory
    del bart_emotion_learner.model
    torch.cuda.empty_cache()

    # Save the performance history for later re-use for various visualizations
    performance_history_list = list()
    for key in sorted(performance_history.keys()):
        if ' Loss' in key:
            performance_history_list.extend(performance_history[key])

    performance_history_df = pd.DataFrame(
        {'Loss': np.array(performance_history_list)},
        index=[f'{key} Epoch {index + 1}' for key in sorted(performance_history.keys()) if ' Loss' in key for index in range(len(performance_history[key]))]
    )
    performance_history_df.to_csv('performance_history_loss.csv')

    performance_history_list = list()
    for key in sorted(performance_history.keys()):
        if ' Loss' not in key:
            performance_history_list.extend(performance_history[key])

    performance_history_df = pd.DataFrame(
        {column: np.array(performance_history_list)[:, index] for index, column in enumerate(['Precision', 'Recall', 'F1', 'Accuracy'])},
        index=[f'{key} Epoch {index + 1}' for key in sorted(performance_history.keys()) if ' Loss' not in key for index in range(len(performance_history[key]))]
    )
    performance_history_df.to_csv('performance_history.csv')

    performance_history_df = pd.DataFrame({'Oracle': np.array(bart_emotion_learner.oracle_selection_history_)})
    performance_history_df.to_csv('oracle_selection_history.csv')

    # Plot the F1 score history as a line graph
    f1_history = {key: np.array(performance_history[key])[:, 2] for key in sorted(performance_history.keys()) if ' Loss' not in key}

    colors = ['indianred', 'slategray', 'blue', 'darkgreen', 'darkorange']
    markers = ['o', '^', 's', 'D', '1']
    line_styles = ['dashed', 'dotted', 'dashdot', 'solid', 'loosely dashdotdotted']
    fig, ax = plt.subplots(figsize=(8.5, 6), dpi=300)

    for index, key in enumerate(sorted(f1_history.keys())):
        ax.plot(
            f1_history[key],
            label=key,
            color=colors[index],
            marker=markers[index],
            linestyle=line_styles[index]
        )

    ax.xaxis.set_major_locator(mpl.ticker.MaxNLocator(nbins=5, integer=True))
    ax.yaxis.set_major_locator(mpl.ticker.MaxNLocator(nbins=10))
    ax.yaxis.set_major_formatter(mpl.ticker.PercentFormatter(xmax=1))

    ax.set_ylim(bottom=0, top=1)

    ax.set_title('Incremental weighted F1')
    ax.set_xlabel('Query iteration')
    ax.set_ylabel('Weighted F1 score')

    plt.legend()

    plt.savefig('incremental_f1_weighted_plot.png', bbox_inches='tight')

    # Plot the oracle selection history as a line graph
    fig, ax = plt.subplots(figsize=(8.5, 6), dpi=300)

    ax.plot(
        bart_emotion_learner.oracle_selection_history_,
        color='black',
        marker='o',
        linestyle='solid'
    )

    ax.xaxis.set_major_locator(mpl.ticker.MaxNLocator(nbins=5, integer=True))
    labels = [
        'Least reliable oracle with lowest query cost',
        '3rd most reliable oracle with 3rd highest query cost',
        '2nd most reliable oracle with 2nd highest query cost',
        'Most reliable oracle with highest query cost'
    ]
    ax.set_yticklabels(labels)

    ax.set_title('Oracle Selection History')
    ax.set_xlabel('Query iteration')
    ax.set_ylabel('Oracle #')

    plt.legend()

    plt.savefig('oracle_selection_history_plot.png', bbox_inches='tight')
