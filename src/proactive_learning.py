"""
Proactive Learning with Emotion Detection in non-cooperative discourse
Copyright (C) 2022  Rigved Rakshit

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU Affero General Public License as published
by the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU Affero General Public License for more details.

You should have received a copy of the GNU Affero General Public License
along with this program.  If not, see <https://www.gnu.org/licenses/>.
"""


import os
import sys
from tqdm.auto import tqdm
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
from sklearn.base import BaseEstimator, ClassifierMixin
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.utils.validation import check_is_fitted
from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from transformers import get_scheduler, AdamW
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
import torch
from modAL.models import ActiveLearner
from modAL.uncertainty import uncertainty_sampling, margin_sampling, entropy_sampling


class EmotionsDataset(Dataset):
    """
    Convert the data into a map-style dataset that can be plugged
    into a data loader for the transformer model.
    """
    def __init__(
            self,
            text,
            labels=None,
            tokenizer=AutoTokenizer.from_pretrained('distilbert-base-uncased')
    ):
        """
        Tokenize the variable-length text as a list of variable-length
        tokenized tensors; and store the corresponding labels.

        :param text: The question and response text.
        :param labels: The labels for the given text, represented as a list of
                       one-hot encoded vectors.
        :param tokenizer: The pretrained tokenizer associated with the
                          pretrained model that will be applied on this dataset.
                          Default checkpoint: distilbert-base-uncased
        """
        if labels is not None:
            self.encodings = {
                'input_ids': list(),
                'attention_mask': list(),
                'labels': list()
            }
        else:
            self.encodings = {
                'input_ids': list(),
                'attention_mask': list(),
            }

        # Padding is delayed until a batch is generated by the
        # data loader, at which point, dynamic padding is applied.
        # For this reason, each sentence of the text needs to be
        # tokenized individually.
        for data_index in range(text.shape[0]):
            tokenized_text = tokenizer(
                text.iloc[data_index],
                return_token_type_ids=False,
                return_attention_mask=True,
                return_tensors='pt',
                truncation=True,
                padding=False
            )

            self.encodings['input_ids'].append(tokenized_text['input_ids'])
            self.encodings['attention_mask'].append(tokenized_text['attention_mask'])

            if labels is not None:
                self.encodings['labels'].append(list(labels.iloc[data_index]))

    def __len__(self):
        """
        Calculate the total number of text documents.

        :return: The total number of text documents.
        """
        return len(self.encodings['input_ids'])

    def __getitem__(self, idx):
        """
        Allow Python-style [] indexing.

        :param idx: Index of the item to retrieve.

        :return: The requested item from the list of tokenized texts.
        """
        return {k: v[idx] for k, v in self.encodings.items()}


class EmotionClassifier(BaseEstimator, ClassifierMixin):
    """
    Multi-label classifier for emotions that fine-tunes hugging face pre-trained transformer model checkpoints.
    This class is intended to be used as the base estimator of a modAL Active Learner. This class provides a
    scikit-learn API to the underlying transformer model.
    """
    def __init__(
            self,
            plutchik_emotions=[
                'Acceptance',
                'Aggressiveness',
                'Annoyance',
                'Apprehension',
                'Awe',
                'Boredom',
                'Contempt',
                'Disapproval',
                'Distraction',
                'Friendliness',
                'Hostility',
                'Interest',
                'Optimism',
                'Pensiveness',
                'Remorse',
                'Serenity',
                'Submission',
            ],
            oracles=[
                'models/distilbert/easy_to_difficult_sampling',
                'models/distilbert/entropy_sampling',
                'models/distilbert/margin_sampling',
                'models/distilbert/uncertainty_sampling'
            ],
            reliability_scores=[
                0.0,
                0.0,
                0.0,
                0.0
            ],
            proactive_learning_model_checkpoint='models/distilbert/proactive_learning',
            tokenizer=AutoTokenizer.from_pretrained('distilbert-base-uncased'),
            model=AutoModelForSequenceClassification.from_pretrained(
                'distilbert-base-uncased',
                num_labels=17,
                problem_type='multi_label_classification'
            ),
            device=torch.device('cuda:0'),
            batch_size=8,
            lr=5e-5,
            num_warmup_steps=0,
            n_iter=5,
            verbose=True
    ):
        """
        Initialize the underlying hugging face transformer model checkpoint.

        :param plutchik_emotions: Emotion labels from Plutchik's wheel of emotions.
        :param oracles: The list of Proactive Learning oracles. Not used in the Active Learning case.
        :param reliability_scores: The reliability scores of each of the Proactive Learning oracles.
                                   Not used in the Active Learning case.
        :param proactive_learning_model_checkpoint: The current Proactive Learning model's checkpoint name.
                                                    This helps to load the model after it was unloaded
                                                    temporarily to accommodate the oracle's model on the
                                                    single instance of the GPU.
                                                    Not used in the Active Learning case.
        :param tokenizer: The underlying model's tokenizer. Default checkpoint: distilbert-base-uncased.
        :param model: The underlying model. Default checkpoint: distilbert-base-uncased.
                      By default, the model's classification head is configured to output 17 labels as defined
                      in the list of chosen Plutchik emotions.
        :param device: The CUDA-enabled GPU device to use for this model. Default: 1st available CUDA-enabled GPU.
        :param batch_size: The number of instances in each batch.
                           Default: 8 (This batch size fits into the memory offered by an NVIDIA P4 GPU.)
        :param lr: Learning rate for the transformer head's optimizer. Default: 5e-5
        :param num_warmup_steps: Number of warmup steps for the learning rate scheduler during training.
        :param n_iter: Number of epochs for the training. Default: 3
        :param verbose: Output training progress and training loss if this is True.
        """
        self.plutchik_emotions = plutchik_emotions
        self.oracles = oracles
        self.reliability_scores = np.array(reliability_scores)
        self.proactive_learning_model_checkpoint = proactive_learning_model_checkpoint
        self.tokenizer = tokenizer
        self.device = device
        self.model = model
        self.model.to(self.device)
        self.batch_size = batch_size
        self.lr = lr
        self.n_iter = n_iter
        self.num_warmup_steps = num_warmup_steps
        self.verbose = verbose

    def fit(self, X, y):
        """
        Incrementally fine-tune the underlying transformer model. This class deviates from the scikit-learn guidelines
        because it doesn't fit a new model on every call to this method. Ideally, `partial_fit` should be called for
        incremental learning. However, modAL calls the `fit` method instead of the `partial_fit` method. Hence, the
        deviation.

        :param X: A Pandas series consisting of the question and response text.
        :param y: A Pandas series for the corresponding emotion labels (one-hot encoded multi-label array).

        :return: Fitted model.
        """
        # Store the training loss so that it can be used later
        self.training_loss_ = getattr(self, 'training_loss_', list())

        # Store the oracle selection history for review later
        self.oracle_selection_history_ = getattr(self, 'oracle_selection_history_', list())

        # Default optimizer for the training
        optimizer = AdamW(self.model.parameters(), lr=self.lr)

        # Create the data loader for the training instances.
        dataloader_train = DataLoader(
            EmotionsDataset(
                text=X,
                labels=y,
                tokenizer=self.tokenizer
            ),
            shuffle=True,
            batch_size=self.batch_size,
            collate_fn=self.collate_batch
        )

        # Number of steps for the training
        num_training_steps = self.n_iter * len(dataloader_train)

        # Learning rate scheduler
        lr_scheduler = get_scheduler(
            'linear',
            optimizer=optimizer,
            num_warmup_steps=self.num_warmup_steps,
            num_training_steps=num_training_steps
        )

        if self.verbose:
            # Set up the progress bar
            progress_bar = tqdm(range(num_training_steps))

        # Train the model
        self.model.train()
        for epoch in range(self.n_iter):
            epoch_loss = 0.0

            for batch in dataloader_train:
                outputs = self.model(**batch)
                optimizer.zero_grad()
                outputs.loss.backward()
                optimizer.step()
                lr_scheduler.step()
                epoch_loss += outputs.loss.item()

                if self.verbose:
                    progress_bar.update(1)

            epoch_loss = epoch_loss / len(dataloader_train)
            self.training_loss_.append(epoch_loss)

            if self.verbose:
                print(f'Epoch #{epoch + 1} loss: {epoch_loss}')

        return self

    def predict(self, X):
        """
        Predict the emotion detected in the given text.

        :param X: A Pandas series consisting of the question and response text.

        :return: Predicted labels.
        """
        # Ensure that fit has been called before attempting predictions
        check_is_fitted(
            self,
            attributes=[
                'training_loss_',
                'oracle_selection_history_'
            ]
        )

        # Find the top 2 most likely labels
        probabilities = self.predict_proba(X)

        return (np.array(self.plutchik_emotions, dtype=np.str_)[np.argpartition(a=probabilities, kth=-2, axis=1)[:, -2:]]).tolist()

    def predict_proba(self, X):
        """
        Calculate the probabilities of the labels for the given text. These are multi-label probabilities.

        :param X: A Pandas series consisting of the question and response text.

        :return: Probabilities for each label, with shape (X.shape[0], n_classes)
        """
        # Ensure that fit has been called before attempting predictions
        check_is_fitted(
            self,
            attributes=[
                'training_loss_',
                'oracle_selection_history_'
            ]
        )

        # Create the dataloader for the testing instances
        dataloader_test = DataLoader(
            EmotionsDataset(
                text=X,
                tokenizer=self.tokenizer
            ),
            shuffle=False,
            batch_size=self.batch_size,
            collate_fn=self.collate_batch
        )

        # Calculate the prediction logits
        logits = list()

        self.model.eval()
        for batch in dataloader_test:
            with torch.no_grad():
                outputs = self.model(**batch)
                logits.append(outputs.logits)

        # Apply the sigmoid function on the output logits so that each output
        # is scaled between 0 and 1 independent of the other outputs (multi-label).
        return torch.sigmoid(torch.cat(logits, 0)).detach().cpu().numpy()

    def collate_batch(self, batch):
        """
        Data collator function that takes a batch of variable-length tokenized
        tensors (variable-length tokenized input strings of sizes 'size_1',
        'size_2', ..., 'size_n') and pads them to be the same length as the
        largest sequence of tokens in the batch. Here, 'n' is the current
        batch size.
        Since tensors on GPUs need to be of the same length, the input
        variable-length tensors are stored on the CPU as a list of tensors,
        padded, and then moved to the first available GPU.

        :param batch: List of variable length tokenized text. Represented as
                      batch = [
                        {
                            'input_ids': Tensor[1, size_1],
                            'attention_mask': Tensor[1, size_1],
                            'labels: Tensor[1]'
                        },
                        {
                            'input_ids': Tensor[1, size_2],
                            'attention_mask': Tensor[1, size_2],
                            'labels: Tensor[1]'
                        },
                        ...,
                        {
                            'input_ids': Tensor[1, size_n],
                            'attention_mask': Tensor[1, size_n],
                            'labels: Tensor[1]'
                        }
                      ]
        :return: Tokenized text with padding. Represented as
                 return {
                    'input_ids': Tensor[batch_size, max_padded_size_of_this_batch],
                    'attention_mask': Tensor[batch_size, max_padded_size_of_this_batch],
                    'labels': Tensor[batch_size]
                 }
        """
        # Extract all the 'input_ids', 'attention_mask', and 'labels' tensors for all the items in this batch
        input_ids = list()
        attention_mask = list()
        labels = list()

        for item in batch:
            # The extracted tensors are 1-D row tensors.
            # However, the function in Torch only pads 1-D column tensors.
            # Therefore, transpose the 1-D row tensors into 1-D column tensors.
            input_ids.append(torch.transpose(item['input_ids'], 0, 1))
            attention_mask.append(torch.transpose(item['attention_mask'], 0, 1))

            if 'labels' in item.keys():
                labels.append(item['labels'])

        # After padding the tensors to the same length, re-transpose them back into 1-D row tensors.
        input_ids = torch.transpose(pad_sequence(input_ids, batch_first=True, padding_value=0), 1, 2)
        attention_mask = torch.transpose(pad_sequence(attention_mask, batch_first=True, padding_value=0), 1, 2)

        # The HuggingFace DataLoader expects tensors of shape [batch_size, max_padded_size_of_this_batch]
        # for the 'input_ids' and the 'attention_mask'. Therefore, reshape the Tensors from
        # [batch_size, 1, max_padded_size_of_this_batch] to [batch_size, max_padded_size_of_this_batch].
        # This is required because we originally used a list of variable length 1-D row tensors.
        padded_data = {
            'input_ids': torch.reshape(input_ids, (input_ids.shape[0], input_ids.shape[2])).to(self.device),
            'attention_mask': torch.reshape(attention_mask, (input_ids.shape[0], attention_mask.shape[2])).to(self.device)
        }

        if len(labels) != 0:
            padded_data['labels'] = torch.tensor(labels).float().to(self.device)

        return padded_data

    def score(self, y_true, y_predictions, **kwargs):
        """
        Calculate metrics for multilabel classification. Metrics calculated are:
            * Exact Match Ratio
            * 0/1 Loss
            * Hamming score (Subset Accuracy)
            * Hamming Loss
            * Precision
            * Recall
            * F1-Measure
        Since the sklearn versions don't work with this dataset, these metrics are manually calculated.

        :param y_predictions: The predictions from the model.
        :param y_true: The gold standard labels.
        :param kwargs: Keyword arguments for the "ClassifierMinix.score" method.
                       By default, it can accept optional array-like sample
                       weights of shape (n_samples,). Default: None
                       This is currently not used in this method.

        :return: Metrics.
        """
        # Definition of the various metrics:
        #   * The Exact Match Ratio is the ratio of completely accurate (ignoring partially accurate) predictions.
        #   * 0/1 Loss: It is the proportion of instances whose predicted value is different from their actual value.
        #   * The Hamming score for each instance is defined as the proportion of the predicted correct labels
        #     to the total number of labels for that instance.
        #   * Hamming Loss is the number of times on average that the relevance of an instance to a class label is
        #     incorrectly predicted.
        #   * Precision is the proportion of correctly predicted labels to the total number of labels predicted,
        #     averaged over all instances.
        #   * Recall is the proportion of correctly predicted labels to the total number of true labels, averaged over
        #     all instances.
        #   * F1-Measure is the harmonic mean of Precision and Recall.

        # Calculate metrics
        metrics = {
            'Exact Match Ratio': np.all(y_true == y_predictions, axis=1).mean(),
            '0/1 Loss': np.any(y_true != y_predictions, axis=1).mean(),
            'Hamming Score': ((y_true.astype(np.int64) & y_predictions.astype(np.int64)).sum(axis=1) / (y_true.astype(np.int64) | y_predictions.astype(np.int64)).sum(axis=1)).mean(),
            'Hamming Loss': ((np.size((y_true.astype(np.int64) == y_predictions.astype(np.int64)), axis=1) - np.count_nonzero((y_true.astype(np.int64) == y_predictions.astype(np.int64)), axis=1)) / (y_true.shape[0] * y_true.shape[1])).mean(),
            'Precision': ((y_true.astype(np.int64) & y_predictions.astype(np.int64)).sum(axis=1) / y_predictions.sum(axis=1)).mean(),
            'Recall': ((y_true.astype(np.int64) & y_predictions.astype(np.int64)).sum(axis=1) / y_true.sum(axis=1)).mean(),
            'F1-Measure': 2 * ((y_true.astype(np.int64) & y_predictions.astype(np.int64)).sum(axis=1) / (y_true.sum(axis=1) + y_predictions.sum(axis=1))).mean()
        }

        # Account for np.nan
        if metrics['Exact Match Ratio'] != metrics['Exact Match Ratio']:
            metrics['Exact Match Ratio'] = 0.0

        if metrics['0/1 Loss'] != metrics['0/1 Loss']:
            metrics['0/1 Loss'] = 1.0

        if metrics['Hamming Score'] != metrics['Hamming Score']:
            metrics['Hamming Score'] = 0.0

        if metrics['Hamming Loss'] != metrics['Hamming Loss']:
            metrics['Hamming Loss'] = 1.0

        if metrics['Precision'] != metrics['Precision']:
            metrics['Precision'] = 0.0

        if metrics['Recall'] != metrics['Recall']:
            metrics['Recall'] = 0.0

        if metrics['F1-Measure'] != metrics['F1-Measure']:
            metrics['F1-Measure'] = 0.0

        return metrics


def easy_to_difficult_query_strategy(
        classifier,
        instances,
        n_instances=65,
):
    """
    Custom Active Learning instance selection strategy. This query finds the utility
    of each instance in the pool and picks the most informative instances to use in the
    next iteration of training. Instances that are easier to label are returned first.

    :param classifier: The Active Learner. Since this method automatically gets added to
                       the Active Learner class, this parameter is the equivalent of
                       'self' for this Active Learner method.
    :param instances: The list of training instances in the pool from which the
                      next most informative instances will be picked.
    :param n_instances: Number of informative instances to pick. Default: 65

    :return: The indices of the instances from X chosen to be labelled
             and the instances from X chosen to be labelled.
    """
    # Find the next set of informative instances for the training from the given pool split
    prediction_probabilities = classifier.estimator.predict_proba(instances)
    sorted_probability_indexes = np.amax(prediction_probabilities, axis=1).argsort()

    return sorted_probability_indexes[-n_instances:]


def random_query_strategy(
        classifier,
        instances,
        n_instances=65,
):
    """
    Custom Active Learning instance selection strategy. This query finds the utility
    of each instance in the pool and picks the most informative instances to use in the
    next iteration of training. Instances are randomly picked.

    :param classifier: The Active Learner. Since this method automatically gets added to
                       the Active Learner class, this parameter is the equivalent of
                       'self' for this Active Learner method.
    :param instances: The list of training instances in the pool from which the
                      next most informative instances will be picked.
    :param n_instances: Number of informative instances to pick. Default: 65

    :return: The indices of the instances from X chosen to be labelled
             and the instances from X chosen to be labelled.
    """
    # Randomly pick the next set of informative instances for the training from the given pool split
    return np.random.choice(range(len(instances)), n_instances)


def run_proactive_learning_experiment(
        learner,
        instances_x_pool,
        instances_x_val,
        instances_y_val,
        n_query_iterations=25,
        n_instances_pool=65
):
    """
    Proactive Learning main loop. It picks instances to learn and iteratively trains
    the Proactive learner.

    :param learner: The Proactive Learner that'll be further trained.
    :param instances_x_pool: The pool of instances to use for further training.
    :param instances_x_val: The validation split used for evaluation.
    :param instances_y_val: The labels of the validation split used for evaluation.
    :param n_query_iterations: Number of queries for the Active Learning experiment. Default: 25
    :param n_instances_pool: Number of instances from the pool split to pick in each Active Learning iteration.
                             Default: 65

    :return: Hamming score on the validation data split for each Proactive Learning training iteration.
    """
    # Convert labels to one-hot encoded vectors
    multilabel_binarizer = MultiLabelBinarizer(classes=learner.estimator.plutchik_emotions)
    multilabel_binarizer.fit([learner.estimator.plutchik_emotions])

    # Store the performance history
    learner_performance_history = {
        'Exact Match Ratio': list(),
        '0/1 Loss': list(),
        'Hamming Score': list(),
        'Hamming Loss': list(),
        'Precision': list(),
        'Recall': list(),
        'F1-Measure': list()
    }

    # Use the least reliable available oracle at the start, in order to save costs
    current_best_oracle_index = 0
    learner.estimator.oracle_selection_history_.append(current_best_oracle_index)

    for index in range(n_query_iterations):
        if learner.estimator.verbose:
            print(f'Proactive Learning Epoch {index + 1}...')

        # Find the next informative indexes to use for training
        query_indexes, _ = learner.query(
            instances_x_pool,
            n_instances=n_instances_pool
        )

        # Find the current best oracle
        current_best_oracle = np.argpartition(learner.estimator.reliability_scores, current_best_oracle_index)[current_best_oracle_index]

        # Swap the current model with the selected oracle's model
        learner.estimator.model.save_pretrained(learner.estimator.proactive_learning_model_checkpoint)
        del learner.estimator.model
        learner.estimator.model = AutoModelForSequenceClassification.from_pretrained(learner.estimator.oracles[current_best_oracle])
        learner.estimator.model.to(learner.estimator.device)

        # Query the selected oracle for the labels
        y_pool_pred = pd.Series(list(multilabel_binarizer.transform(learner.predict(instances_x_pool.iloc[query_indexes]))))

        # Re-load the Proactive Learning model
        del learner.estimator.model
        learner.estimator.model = AutoModelForSequenceClassification.from_pretrained(learner.estimator.proactive_learning_model_checkpoint )
        learner.estimator.model.to(learner.estimator.device)

        # Fine-tune the Proactive Learner using the selected instance(s)
        learner.teach(
            X=instances_x_pool.iloc[query_indexes],
            y=y_pool_pred,
            only_new=True
        )

        # Store the performance of the learner after the new learning phase.
        # Use the validation dataset for this purpose.
        performance = learner.score(
            np.stack(
                np.array(
                    instances_y_val.tolist()
                )
            ).astype(np.int),
            np.stack(
                multilabel_binarizer.transform(
                    learner.predict(instances_x_val)
                )
            ).astype(np.int)
        )

        for metric_key in performance.keys():
            learner_performance_history[metric_key].append(performance[metric_key])

        # Pick the next oracle such that the cost is minimized using a greedy approach
        if performance['F1-Measure'] > learner.estimator.reliability_scores[current_best_oracle]:
            # If the current model performed better than the oracle, then choose a less
            # reliable oracle next time to save costs
            if current_best_oracle_index != 0:
                current_best_oracle_index -= 1
        elif performance['F1-Measure'] < learner.estimator.reliability_scores[current_best_oracle]:
            # If the current model performed worse than the oracle, then choose a more
            # reliable oracle next time
            if current_best_oracle_index != (learner.estimator.reliability_scores.shape[0] - 1):
                current_best_oracle_index += 1

        learner.estimator.oracle_selection_history_.append(current_best_oracle_index)

        if learner.estimator.verbose:
            print(f'Learner Performance at {index + 1}: {learner_performance_history["F1-Measure"][-1]}')

        # Sampling without replacement
        instances_x_pool.drop(query_indexes, inplace=True)
        instances_x_pool.reset_index(drop=True, inplace=True)

    return learner_performance_history


def run_active_learning_experiment(
        learner,
        instances_x_pool,
        instances_y_pool,
        instances_x_val,
        instances_y_val,
        n_query_iterations=25,
        n_instances_pool=65
):
    """
    Active Learning main loop. It picks instances to learn and iteratively trains
    the Active learner.

    :param learner: The Active Learner that'll be further trained.
    :param instances_x_pool: The pool of instances to use for further training.
    :param instances_y_pool: The pool of instance labels to use for further training.
    :param instances_x_val: The validation split used for evaluation.
    :param instances_y_val: The labels of the validation split used for evaluation.
    :param n_query_iterations: Number of queries for the Active Learning experiment. Default: 25
    :param n_instances_pool: Number of instances from the pool split to pick in each Active Learning iteration.
                             Default: 65

    :return: Precision, Recall, F1 score, and Accuracy on the validation data split
             after the current Active Learning iteration completes.
    """
    # Convert labels to one-hot encoded vectors
    multilabel_binarizer = MultiLabelBinarizer(classes=learner.estimator.plutchik_emotions)
    multilabel_binarizer.fit([learner.estimator.plutchik_emotions])

    # Store the performance history
    learner_performance_history = {
        'Exact Match Ratio': list(),
        '0/1 Loss': list(),
        'Hamming Score': list(),
        'Hamming Loss': list(),
        'Precision': list(),
        'Recall': list(),
        'F1-Measure': list()
    }

    for index in range(n_query_iterations):
        if learner.estimator.verbose:
            print(f'Active Learning Epoch {index + 1}...')

        # Find the next informative indexes to use for training
        query_indexes, _ = learner.query(
            instances_x_pool,
            n_instances=n_instances_pool
        )

        # Fine-tune the Active Learner using the selected instance(s)
        learner.teach(
            X=instances_x_pool.iloc[query_indexes],
            y=instances_y_pool.iloc[query_indexes],
            only_new=True
        )

        # Store the performance of the learner after the new learning phase.
        # Use the validation dataset for this purpose.
        performance = learner.score(
            np.stack(
                np.array(instances_y_val.tolist())
            ).astype(np.int),
            np.stack(
                multilabel_binarizer.transform(
                    learner.predict(instances_x_val)
                )
            ).astype(np.int)
        )

        for metric_key in performance.keys():
            learner_performance_history[metric_key].append(performance[metric_key])

        if learner.estimator.verbose:
            print(f'Learner Performance at {index + 1}: {learner_performance_history["F1-Measure"][-1]}')

        # Sampling without replacement
        instances_x_pool.drop(query_indexes, inplace=True)
        instances_y_pool.drop(query_indexes, inplace=True)
        instances_x_pool.reset_index(drop=True, inplace=True)
        instances_y_pool.reset_index(drop=True, inplace=True)

    return learner_performance_history


if __name__ == '__main__':
    # Disable parallelism in the tokenizers because it does not support forking the parent
    # Python process after the tokenizers have already been run at least once.
    # This does not cause any performance issues because the dataset is very small.
    os.environ['TOKENIZERS_PARALLELISM'] = 'false'

    # Set options for pandas display
    pd.set_option('display.max_columns', None)
    pd.set_option('display.max_colwidth', None)
    pd.set_option('display.max_seq_item', None)
    pd.set_option('display.max_rows', None)
    pd.set_option('display.precision', 2)

    # Seed for various random number generators
    seed = 42

    # The BART model needs the memory offered by at least an NVIDIA A100 GPU
    if torch.cuda.is_available():
        # Set the PyTorch seed for reproducibility
        torch.cuda.manual_seed_all(seed)
    else:
        print('The underlying DistilBERT-Base-Uncased model\'s training needs the memory offered '
              'by at least an NVIDIA P4 GPU!')
        sys.exit(1)

    # Set the Numpy random generator seed for reproducibility
    np.random.seed(seed)

    # Load the dataset
    raw_dataset = pd.read_csv('../data/emotion_annotated_data.csv')

    # Extract the required features and split into train, pool, validation, and test datasets
    data = pd.DataFrame({
        'text': pd.concat(
            [
                raw_dataset['question'],
                raw_dataset['response']
            ],
            ignore_index=True
        ),
        'emotion1': pd.concat(
            [
                raw_dataset['q_emotion1'],
                raw_dataset['r_emotion1']
            ],
            ignore_index=True
        ),
        'emotion2': pd.concat(
            [
                raw_dataset['q_emotion2'],
                raw_dataset['r_emotion2']
            ],
            ignore_index=True
        )
    })
    data['labels'] = list(zip(data['emotion1'], data['emotion2']))
    data.drop(columns=['emotion1', 'emotion2'], inplace=True)

    plutchik_emotions = [
        'Acceptance',
        'Aggressiveness',
        'Annoyance',
        'Apprehension',
        'Awe',
        'Boredom',
        'Contempt',
        'Disapproval',
        'Distraction',
        'Friendliness',
        'Hostility',
        'Interest',
        'Optimism',
        'Pensiveness',
        'Remorse',
        'Serenity',
        'Submission',
    ]
    mlb = MultiLabelBinarizer(classes=plutchik_emotions)
    mlb.fit([plutchik_emotions])

    data['labels'] = list(mlb.transform(data['labels']))

    # Split the dataset into training, pool, validation, and testing datasets
    x_train, x_val, y_train, y_val = train_test_split(
        data['text'],
        data['labels'],
        train_size=0.9,
        random_state=42
    )
    x_train, x_test, y_train, y_test = train_test_split(
        x_train,
        y_train,
        train_size=0.9,
        random_state=42
    )
    x_train, x_pool, y_train, y_pool = train_test_split(
        x_train,
        y_train,
        train_size=0.1,
        random_state=42
    )

    x_train.reset_index(drop=True, inplace=True)
    y_train.reset_index(drop=True, inplace=True)
    x_pool.reset_index(drop=True, inplace=True)
    y_pool.reset_index(drop=True, inplace=True)
    x_val.reset_index(drop=True, inplace=True)
    y_val.reset_index(drop=True, inplace=True)
    x_test.reset_index(drop=True, inplace=True)
    y_test.reset_index(drop=True, inplace=True)

    # We restrict to using a single GPU so that the SLURM job running this experiment gets picked up faster.
    # We will test each Active Learning and Proactive Learning strategy one-by-one on the same dataset splits
    # and then compare their performance.
    # Store the performance of the oracles with each iteration of Active Learning
    performance_history = {
        'Easy-to-Difficult Sampling': dict(),
        'Easy-to-Difficult Sampling Loss': list(),
        'Entropy Sampling': dict(),
        'Entropy Sampling Loss': list(),
        'Margin Sampling': dict(),
        'Margin Sampling Loss': list(),
        'Uncertainty Sampling': dict(),
        'Uncertainty Sampling Loss': list(),
        'Proactive Learning': dict(),
        'Proactive Learning Loss': list()
    }

    # Proactive Learning parameters for the number of instances to pick for each query
    # and the number of total queries to perform.
    n_iterations = 25
    n_pool_instances = 65
    oracles_checkpoints = [
        'models/distilbert/easy_to_difficult_sampling',
        'models/distilbert/entropy_sampling',
        'models/distilbert/margin_sampling',
        'models/distilbert/uncertainty_sampling'
    ]
    oracle_reliability_scores = list()
    performance_history_test_split = {
        'Easy-to-Difficult Sampling': dict(),
        'Entropy Sampling': dict(),
        'Margin Sampling': dict(),
        'Uncertainty Sampling': dict(),
        'Proactive Learning': dict()
    }

    # Clear GPU memory
    torch.cuda.empty_cache()

    # Strategy 1: Easy-to-Difficult Sampling
    print('Current Active Learning technique: Easy-to-Difficult Sampling')

    distilbert_emotion_classifier = EmotionClassifier()
    distilbert_emotion_classifier.training_loss_ = list()
    distilbert_emotion_classifier.oracle_selection_history_ = list()
    distilbert_active_learner = ActiveLearner(
        estimator=distilbert_emotion_classifier,
        query_strategy=easy_to_difficult_query_strategy,
        X_training=x_train,
        y_training=y_train
    )

    performance_metrics = run_active_learning_experiment(
        learner=distilbert_active_learner,
        instances_x_pool=x_pool.copy(deep=True),
        instances_y_pool=y_pool.copy(deep=True),
        instances_x_val=x_val,
        instances_y_val=y_val,
        n_query_iterations=n_iterations,
        n_instances_pool=n_pool_instances
    )

    for metric_key in performance_metrics.keys():
        performance_history['Easy-to-Difficult Sampling'][metric_key] = performance_metrics[metric_key]

    performance_history['Easy-to-Difficult Sampling Loss'].extend(distilbert_emotion_classifier.training_loss_)
    oracle_reliability_scores.append(performance_history['Easy-to-Difficult Sampling']['F1-Measure'][-1])

    performance_metrics_test_split = distilbert_emotion_classifier.score(
        np.stack(
            np.array(y_test.tolist())
        ).astype(np.int),
        np.stack(
            mlb.transform(
                distilbert_emotion_classifier.predict(x_test)
            )
        ).astype(np.int)
    )

    for metric_key in performance_metrics_test_split.keys():
        performance_history_test_split['Easy-to-Difficult Sampling'][metric_key] = performance_metrics_test_split[metric_key]

    print('Performance History:')
    print(performance_history)
    print('Performance History on Test Dataset:')
    print(performance_history_test_split)

    # Clear GPU memory
    distilbert_emotion_classifier.model.save_pretrained(oracles_checkpoints[0])
    del distilbert_emotion_classifier.model
    torch.cuda.empty_cache()

    # Strategy 2: Entropy Sampling
    print('Current Active Learning technique: Entropy Sampling')

    distilbert_emotion_classifier = EmotionClassifier()
    distilbert_active_learner = ActiveLearner(
        estimator=distilbert_emotion_classifier,
        query_strategy=entropy_sampling,
        X_training=x_train,
        y_training=y_train
    )

    performance_metrics = run_active_learning_experiment(
        learner=distilbert_active_learner,
        instances_x_pool=x_pool.copy(deep=True),
        instances_y_pool=y_pool.copy(deep=True),
        instances_x_val=x_val,
        instances_y_val=y_val,
        n_query_iterations=n_iterations,
        n_instances_pool=n_pool_instances
    )

    for metric_key in performance_metrics.keys():
        performance_history['Entropy Sampling'][metric_key] = performance_metrics[metric_key]

    performance_history['Entropy Sampling Loss'].extend(distilbert_emotion_classifier.training_loss_)
    oracle_reliability_scores.append(performance_history['Entropy Sampling']['F1-Measure'][-1])

    performance_metrics_test_split = distilbert_emotion_classifier.score(
        np.stack(
            np.array(y_test.tolist())
        ).astype(np.int),
        np.stack(
            mlb.transform(
                distilbert_emotion_classifier.predict(x_test)
            )
        ).astype(np.int)
    )

    for metric_key in performance_metrics_test_split.keys():
        performance_history_test_split['Entropy Sampling'][metric_key] = performance_metrics_test_split[metric_key]

    print('Performance History:')
    print(performance_history)
    print('Performance History on Test Dataset:')
    print(performance_history_test_split)

    # Clear GPU memory
    distilbert_emotion_classifier.model.save_pretrained(oracles_checkpoints[1])
    del distilbert_emotion_classifier.model
    torch.cuda.empty_cache()

    # Strategy 3: Margin Sampling
    print('Current Active Learning technique: Margin Sampling')

    distilbert_emotion_classifier = EmotionClassifier()
    distilbert_active_learner = ActiveLearner(
        estimator=distilbert_emotion_classifier,
        query_strategy=margin_sampling,
        X_training=x_train,
        y_training=y_train
    )

    performance_metrics = run_active_learning_experiment(
        learner=distilbert_active_learner,
        instances_x_pool=x_pool.copy(deep=True),
        instances_y_pool=y_pool.copy(deep=True),
        instances_x_val=x_val,
        instances_y_val=y_val,
        n_query_iterations=n_iterations,
        n_instances_pool=n_pool_instances
    )

    for metric_key in performance_metrics.keys():
        performance_history['Margin Sampling'][metric_key] = performance_metrics[metric_key]

    performance_history['Margin Sampling Loss'].extend(distilbert_emotion_classifier.training_loss_)
    oracle_reliability_scores.append(performance_history['Margin Sampling']['F1-Measure'][-1])

    performance_metrics_test_split = distilbert_emotion_classifier.score(
        np.stack(
            np.array(y_test.tolist())
        ).astype(np.int),
        np.stack(
            mlb.transform(
                distilbert_emotion_classifier.predict(x_test)
            )
        ).astype(np.int)
    )

    for metric_key in performance_metrics_test_split.keys():
        performance_history_test_split['Margin Sampling'][metric_key] = performance_metrics_test_split[metric_key]

    print('Performance History:')
    print(performance_history)
    print('Performance History on Test Dataset:')
    print(performance_history_test_split)

    # Clear GPU memory
    distilbert_emotion_classifier.model.save_pretrained(oracles_checkpoints[2])
    del distilbert_emotion_classifier.model
    torch.cuda.empty_cache()

    # Strategy 4: Uncertainty Sampling
    print('Current Active Learning technique: Uncertainty Sampling')

    distilbert_emotion_classifier = EmotionClassifier()
    distilbert_active_learner = ActiveLearner(
        estimator=distilbert_emotion_classifier,
        query_strategy=uncertainty_sampling,
        X_training=x_train,
        y_training=y_train
    )

    performance_metrics = run_active_learning_experiment(
        learner=distilbert_active_learner,
        instances_x_pool=x_pool.copy(deep=True),
        instances_y_pool=y_pool.copy(deep=True),
        instances_x_val=x_val,
        instances_y_val=y_val,
        n_query_iterations=n_iterations,
        n_instances_pool=n_pool_instances
    )

    for metric_key in performance_metrics.keys():
        performance_history['Uncertainty Sampling'][metric_key] = performance_metrics[metric_key]

    performance_history['Uncertainty Sampling Loss'].extend(distilbert_emotion_classifier.training_loss_)
    oracle_reliability_scores.append(performance_history['Uncertainty Sampling']['F1-Measure'][-1])

    performance_metrics_test_split = distilbert_emotion_classifier.score(
        np.stack(
            np.array(y_test.tolist())
        ).astype(np.int),
        np.stack(
            mlb.transform(
                distilbert_emotion_classifier.predict(x_test)
            )
        ).astype(np.int)
    )

    for metric_key in performance_metrics_test_split.keys():
        performance_history_test_split['Uncertainty Sampling'][metric_key] = performance_metrics_test_split[metric_key]

    print('Performance History:')
    print(performance_history)
    print('Performance History on Test Dataset:')
    print(performance_history_test_split)

    # Clear GPU memory
    distilbert_emotion_classifier.model.save_pretrained(oracles_checkpoints[3])
    del distilbert_emotion_classifier.model
    torch.cuda.empty_cache()

    # Strategy 5: Proactive Learning with Uncertainty Sampling
    print('Current Active Learning technique: Proactive Learning with Random Sampling')

    distilbert_emotion_classifier = EmotionClassifier(
        reliability_scores=oracle_reliability_scores
    )
    distilbert_active_learner = ActiveLearner(
        estimator=distilbert_emotion_classifier,
        query_strategy=random_query_strategy,
        X_training=x_train,
        y_training=y_train
    )

    performance_metrics = run_proactive_learning_experiment(
        learner=distilbert_active_learner,
        instances_x_pool=x_pool.copy(deep=True),
        instances_x_val=x_val.copy(deep=True),
        instances_y_val=y_val,
        n_query_iterations=n_iterations,
        n_instances_pool=n_pool_instances
    )

    for metric_key in performance_metrics.keys():
        performance_history['Proactive Learning'][metric_key] = performance_metrics[metric_key]

    performance_history['Proactive Learning Loss'].extend(distilbert_emotion_classifier.training_loss_)

    performance_metrics_test_split = distilbert_emotion_classifier.score(
        np.stack(
            np.array(y_test.tolist())
        ).astype(np.int),
        np.stack(
            mlb.transform(
                distilbert_emotion_classifier.predict(x_test)
            )
        ).astype(np.int)
    )

    for metric_key in performance_metrics_test_split.keys():
        performance_history_test_split['Proactive Learning'][metric_key] = performance_metrics_test_split[metric_key]

    print('Performance History:')
    print(performance_history)
    print('Performance History on Test Dataset:')
    print(performance_history_test_split)
    print('Oracle Selection History:')
    print(distilbert_emotion_classifier.oracle_selection_history_)

    # Clear GPU memory
    distilbert_emotion_classifier.model.save_pretrained(distilbert_emotion_classifier.proactive_learning_model_checkpoint)
    del distilbert_emotion_classifier.model
    torch.cuda.empty_cache()

    # Save the performance history for later re-use for various visualizations
    performance_history_list = list()
    for key in sorted(performance_history.keys()):
        if ' Loss' in key:
            performance_history_list.extend(performance_history[key])

    performance_history_df = pd.DataFrame(
        {'Loss': np.array(performance_history_list)},
        index=[f'{key} Epoch {index + 1}' for key in sorted(performance_history.keys()) if ' Loss' in key for index in range(len(performance_history[key]))]
    )
    performance_history_df.to_csv('performance_history_loss_distilbert.csv')

    performance_history_lists = {
        'Exact Match Ratio': list(),
        '0/1 Loss': list(),
        'Hamming Score': list(),
        'Hamming Loss': list(),
        'Precision': list(),
        'Recall': list(),
        'F1-Measure': list()
    }

    for sampling_method in sorted(performance_history.keys()):
        if ' Loss' not in sampling_method:
            for metric in sorted(performance_history[sampling_method].keys()):
                performance_history_lists[metric].extend(performance_history[sampling_method][metric])

    performance_history_df = pd.DataFrame(
        {metric: performance_history_lists[metric] for metric in sorted(performance_history_lists.keys())},
        index=[f'{sampling_method} Epoch {index + 1}' for sampling_method in sorted(performance_history.keys()) if ' Loss' not in sampling_method for index in range(len(performance_history[sampling_method]['F1-Measure']))]
    )
    performance_history_df.to_csv('performance_history_scores_distilbert.csv')

    performance_history_df = pd.DataFrame({'Oracle': np.array(distilbert_emotion_classifier.oracle_selection_history_)})
    performance_history_df.to_csv('oracle_selection_history_distilbert.csv')

    # Plot the Hamming score history and oracle selection history as a line graph
    fig, ax = plt.subplots(figsize=(8.5, 6), dpi=300)

    line1 = ax.plot(
        distilbert_emotion_classifier.oracle_selection_history_,
        label='Oracle #',
        color='green'
    )

    ax.tick_params(axis='y', labelcolor='green')
    ax.xaxis.set_major_locator(mpl.ticker.MaxNLocator(nbins=5, integer=True))
    ax.set_yticks(range(4))
    ax.set_title('Oracle Selection History')
    ax.set_xlabel('Query iteration')
    ax.set_ylabel('Least reliable oracle (0) to Most reliable oracle (3)')

    ax2 = ax.twinx()

    colors = ['indianred', 'slategray', 'blue', 'darkorange', 'black']
    markers = ['o', '^', 's', 'D', '1']
    line_styles = ['dashed', 'dotted', 'dashdot', (0, (5, 10)), 'solid']

    index = 0
    line2 = list()

    for key in sorted(performance_history.keys()):
        if ' Loss' not in key:
            line2.append(ax2.plot(
                performance_history[key]['F1-Measure'],
                label=key,
                color=colors[index]
            ))

            index += 1

    ax2.tick_params(axis='y', labelcolor='black')
    ax2.set_ylabel('F1-Measure (Sample Weighted)', rotation=270, labelpad=15)

    lines = list()
    lines.append(line1[0])

    for line in line2:
        lines.append(line[0])

    labs = [line.get_label() for line in lines]
    ax.legend(lines, labs)

    ax2.set_ylim(bottom=0.2, top=0.7)
    ax2.yaxis.set_major_locator(mpl.ticker.MaxNLocator(nbins=10))
    ax2.yaxis.set_major_formatter(mpl.ticker.PercentFormatter(xmax=1))

    plt.savefig('oracle_selection_history_plot_distilbert.png', bbox_inches='tight')
